
# Zero-Shot Object Detection with Aya Vision
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/your-repo-path/blob/main/your-notebook.ipynb)

C4AI Aya Vision 8B is an open weights research release of an 8-billion parameter model with advanced capabilities optimized for a variety of vision-language use cases, including OCR, captioning, visual reasoning, summarization, question answering, code, and more. It is a multilingual model trained to excel in 23 languages in vision and language.
## Environment setup
### Configure your API keys

- Open your [`HuggingFace Settings`](https://huggingface.co/settings) page. Click `Access Tokens` then `New Token` to generate new token.

import os
from google.colab import userdata

os.environ["HF_TOKEN"] = userdata.get("HF_TOKEN")
### Check GPU availability

Let's make sure that we have access to GPU. We can use `nvidia-smi` command to do that. In case of any problems navigate to `Edit` -> `Notebook settings` -> `Hardware accelerator`, set it to `T4 GPU`, and then click `Save`.
!nvidia-smi
### Install dependencies

Installs all required python libraries, including [`maestro`](https://github.com/roboflow/maestro) for Qwen2.5-VL and [`supervision`](https://github.com/roboflow/supervision) for visualization.
! pip install 'git+https://github.com/huggingface/transformers.git@v4.49.0-AyaVision'
! pip install supervision opencv-python matplotlib requests numpy

## Load Aya-8B model, and do necessary imports

from transformers import AutoProcessor, AutoModelForImageTextToText
import torch
import requests
import cv2
import numpy as np
import json
import supervision as sv
from matplotlib import pyplot as plt
import re

model_id = "CohereForAI/aya-vision-8b"

processor = AutoProcessor.from_pretrained(model_id)
model = AutoModelForImageTextToText.from_pretrained(
    model_id, device_map="auto", torch_dtype=torch.float16
)
# Zero Shot Object Detection
Loads the Aya-8B model model (and its processor) from Hugging Face, preparing the model for inference.
image_url = "https://www.minneapolisfed.org/-/media/images/pubs/fedgaz/15-04/key-images/working-on-the-railroad-key.jpg"
response = requests.get(image_url, stream=True)
image_array = np.asarray(bytearray(response.content), dtype=np.uint8)
image = cv2.imdecode(image_array, cv2.IMREAD_COLOR)
resolution_wh = (364,364) # Aya Input resolution



messages = [
    {"role": "user",
     "content": [
       {"type": "image", "url": image_url},
        {"type": "text", "text": "Find the persons in the image. Return the bounding box coordinates, x1, y2, x2, y2 in json with keys as the label and values x1, x2, y1, y2"},
    ]},
    ]

inputs = processor.apply_chat_template(
    messages, padding=True, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors="pt"
).to(model.device)

gen_tokens = model.generate(
    **inputs,
    max_new_tokens=300,
    do_sample=True,
    temperature=0.3,
)

text_response = processor.tokenizer.decode(gen_tokens[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
print(text_response)



# Visualise the output
json_match = re.search(r'\{[\s\S]*\}', text_response)
if json_match:
    json_data = json_match.group(0)
    bbox_data = json.loads(json_data)
else:
    raise ValueError("JSON data not found in response.")
# Todo: Scaling of bbox coordinated
bboxes = []
class_ids = []
class_id = 0 # Person
for idx, (key, value) in enumerate(bbox_data.items()):
    x1, y1, x2, y2 = value["x1"], value["y1"], value["x2"], value["y2"]
    bboxes.append([x1, y1, x2, y2])
    class_ids.append(class_id)

bboxes = np.array(bboxes)
class_ids = np.array(class_ids)

bounding_box_annotator = sv.BoxAnnotator(color=sv.Color.RED, thickness=3)
bbox_xyxy = sv.Detections(xyxy=bboxes, class_id=class_ids)
annotated_image = bounding_box_annotator.annotate(scene=image, detections=bbox_xyxy)
annotated_image = cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB)
plt.figure(figsize=(8, 6))
plt.imshow(annotated_image)
plt.axis("off")
plt.show()


# Counting

Analayse the model on counting task

# Format message with the aya-vision chat template
counting_image_url = "https://i1.sndcdn.com/artworks-000631939663-rvxff5-t500x500.jpg"
messages = [
    {"role": "user",
     "content": [
       {"type": "image", "url": counting_image_url},
        {"type": "text", "text": "How many coins are there in the image?"},
    ]},
    ]

inputs = processor.apply_chat_template(
    messages, padding=True, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors="pt"
).to(model.device)

gen_tokens = model.generate(
    **inputs,
    max_new_tokens=300,
    do_sample=True,
    temperature=0.3,
)
response = requests.get(counting_image_url, stream=True)
image_array = np.asarray(bytearray(response.content), dtype=np.uint8)
image = cv2.imdecode(image_array, cv2.IMREAD_COLOR)
plt.figure(figsize=(8, 6))
plt.imshow(image)
plt.axis("off")
plt.show()
print(processor.tokenizer.decode(gen_tokens[0][inputs.input_ids.shape[1]:], skip_special_tokens=True))
# OCR
ocr_image_url = "https://media-cdn.tripadvisor.com/media/photo-s/16/c3/c0/e3/restaurant-receipt.jpg"

messages = [
    {"role": "user",
     "content": [
       {"type": "image", "url": ocr_image_url},
        {"type": "text", "text": "Extract the text from the image and return a structured output, extract the prices, item and quantity"},
    ]},
    ]

inputs = processor.apply_chat_template(
    messages, padding=True, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors="pt"
).to(model.device)

gen_tokens = model.generate(
    **inputs,
    max_new_tokens=300,
    do_sample=True,
    temperature=0.3,
)
response = requests.get(ocr_image_url, stream=True)
image_array = np.asarray(bytearray(response.content), dtype=np.uint8)
image = cv2.imdecode(image_array, cv2.IMREAD_COLOR)
plt.figure(figsize=(8, 6))
plt.imshow(image)
plt.axis("off")
plt.show()
print(processor.tokenizer.decode(gen_tokens[0][inputs.input_ids.shape[1]:], skip_special_tokens=True))
# VQA
vqa_url = "https://raw.githubusercontent.com/haotian-liu/LLaVA/1a91fc274d7c35a9b50b3cb29c4247ae5837ce39/images/llava_v1_5_radar.jpg"
messages = [
    {"role": "user",
     "content": [
       {"type": "image", "url": vqa_url},
        {"type": "text", "text": "Which model has the second best performance in LLAVA-Bench"},
    ]},
    ]

inputs = processor.apply_chat_template(
    messages, padding=True, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors="pt"
).to(model.device)

gen_tokens = model.generate(
    **inputs,
    max_new_tokens=300,
    do_sample=True,
    temperature=0.3,
)
response = requests.get(vqa_url, stream=True)
image_array = np.asarray(bytearray(response.content), dtype=np.uint8)
image = cv2.imdecode(image_array, cv2.IMREAD_COLOR)
plt.figure(figsize=(8, 6))
plt.imshow(image)
plt.axis("off")
plt.show()
print(processor.tokenizer.decode(gen_tokens[0][inputs.input_ids.shape[1]:], skip_special_tokens=True))
# Image captions
image_caption_url = "https://www.minneapolisfed.org/-/media/images/pubs/fedgaz/15-04/key-images/working-on-the-railroad-key.jpg"
messages = [
    {"role": "user",
     "content": [
       {"type": "image", "url": image_caption_url},
        {"type": "text", "text": "Generate a caption for the image"},
    ]},
    ]

inputs = processor.apply_chat_template(
    messages, padding=True, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors="pt"
).to(model.device)

gen_tokens = model.generate(
    **inputs,
    max_new_tokens=300,
    do_sample=True,
    temperature=0.3,
)
response = requests.get(image_caption_url, stream=True)
image_array = np.asarray(bytearray(response.content), dtype=np.uint8)
image = cv2.imdecode(image_array, cv2.IMREAD_COLOR)
image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

plt.figure(figsize=(8, 6))
plt.imshow(image)
plt.axis("off")
plt.show()
print(processor.tokenizer.decode(gen_tokens[0][inputs.input_ids.shape[1]:], skip_special_tokens=True))
